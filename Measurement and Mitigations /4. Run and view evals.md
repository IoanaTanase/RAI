Sources:
- https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-generative-ai-app
- https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-results
- https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/online-evaluation?tabs=windows

# How to evaluate generative AI models and applications with Azure AI Foundry

To thoroughly assess the performance of your generative AI models and applications when applied to a substantial dataset, you can initiate an evaluation process. During this evaluation, your model or application is tested with the given dataset, and its performance will be quantitatively measured with both mathematical based metrics and AI-assisted metrics. This evaluation run provides you with comprehensive insights into the application's capabilities and limitations.

To carry out this evaluation, you can utilize the evaluation functionality in Azure AI Foundry portal, a comprehensive platform that offers tools and features for assessing the performance and safety of your generative AI model. In Azure AI Foundry portal, you're able to log, view, and analyze detailed evaluation metrics.

In this article, you learn to create an evaluation run against model, a test dataset or a flow with built-in evaluation metrics from Azure AI Foundry UI. For greater flexibility, you can establish a custom evaluation flow and employ the custom evaluation feature. Alternatively, if your objective is solely to conduct a batch run without any evaluation, you can also utilize the custom evaluation feature.

## Create an evaluation with built-in evaluation metrics

An evaluation run allows you to generate metric outputs for each data row in your test dataset. You can choose one or more evaluation metrics to assess the output from different aspects. You can create an evaluation run from the evaluation, model catalog or prompt flow pages in Azure AI Foundry portal. Then an evaluation creation wizard appears to guide you through the process of setting up an evaluation run.

### From the evaluate page

From the collapsible left menu, select Evaluation > + Create a new evaluation.

![image](https://github.com/user-attachments/assets/f0b1a43a-cbf9-4520-9eb6-1f74b6cfd728)

### From the model catalog page

From the collapsible left menu, select Model catalog > go to specific model > navigate to the benchmark tab > Try with your own data. This opens the model evaluation panel for you to create an evaluation run against your selected model.

![image](https://github.com/user-attachments/assets/954a31ff-cdf1-4418-ac23-f8ed104e85bd)

### Evaluation target

When you start an evaluation from the evaluate page, you need to decide what is the evaluation target first. By specifying the appropriate evaluation target, we can tailor the evaluation to the specific nature of your application, ensuring accurate and relevant metrics. We support three types of evaluation target:
- Model and prompt: You want to evaluate the output generated by your selected model and user-defined prompt.
- Dataset: You already have your model generated outputs in a test dataset.
- Prompt flow: You have created a flow, and you want to evaluate the output from the flow.

![image](https://github.com/user-attachments/assets/a9ab9670-7b8b-4414-b1d3-998a2a1764d8)

### Dataset or prompt flow evaluation

When you enter the evaluation creation wizard, you can provide an optional name for your evaluation run. We currently offer support for the query and response scenario, which is designed for applications that involve answering user queries and providing responses with or without context information.

You can optionally add descriptions and tags to evaluation runs for improved organization, context, and ease of retrieval.

You can also use the help panel to check the FAQs and guide yourself through the wizard.

![image](https://github.com/user-attachments/assets/76262f2f-1a3c-4048-8db3-5dc80f8c2f5a)

If you're evaluating a prompt flow, you can select the flow to evaluate. If you initiate the evaluation from the Flow page, we'll automatically select your flow to evaluate. If you intend to evaluate another flow, you can select a different one. It's important to note that within a flow, you might have multiple nodes, each of which could have its own set of variants. In such cases, you must specify the node and the variants you wish to assess during the evaluation process.

![image](https://github.com/user-attachments/assets/58aa574d-ec7b-45ce-8b78-82b0c89663a0)

### Configure test data

You can select from pre-existing datasets or upload a new dataset specifically to evaluate. The test dataset needs to have the model generated outputs to be used for evaluation if there's no flow selected in the previous step.
- Choose existing dataset: You can choose the test dataset from your established dataset collection.
![image](https://github.com/user-attachments/assets/654a3068-8fa2-40ee-940c-0f4f741401e8)
- Add new dataset: You can upload files from your local storage. We only support .csv and .jsonl file formats.
![image](https://github.com/user-attachments/assets/4597685f-2712-4103-9d5d-150111860ef4)
- Data mapping for flow: If you select a flow to evaluate, ensure that your data columns are configured to align with the required inputs for the flow to execute a batch run, generating output for assessment. The evaluation will then be conducted using the output from the flow. Then, configure the data mapping for evaluation inputs in the next step.
![image](https://github.com/user-attachments/assets/1cbe6e24-d859-4cef-986d-f8fb6f8b511e)

### Select metrics
We support three types of metrics curated by Microsoft to facilitate a comprehensive evaluation of your application:
- AI quality (AI assisted): These metrics evaluate the overall quality and coherence of the generated content. To run these metrics, it requires a model deployment as judge.
- AI quality (NLP): These NLP metrics are mathematical based, and they also evaluate the overall quality of the generated content. They often require ground truth data, but they don't require model deployment as judge.
- Risk and safety metrics: These metrics focus on identifying potential content risks and ensuring the safety of the generated content.
![image](https://github.com/user-attachments/assets/7d5249d1-7960-4aab-90b0-bc2cdb6bdd63)
You can refer to the table for the complete list of metrics we offer support for in each scenario. For more in-depth information on each metric definition and how it's calculated, see Evaluation and monitoring metrics.

| **AI quality (AI assisted)** | **AI quality (NLP)** | **Risk and safety metrics** |
|------------------------------|----------------------|-----------------------------|
| Groundedness                 | F1 score             | Self-harm-related content   |
| Relevance                    | ROUGE score          | Hateful and unfair content  |
| Coherence                    | BLEU score           | Violent content             |
| Fluency                      | GLEU score           | Sexual content              |
| GPT similarity               | METEOR score         | Protected material          |
|                              |                      | Indirect attack             |

When running AI assisted quality evaluation, you must specify a GPT model for the calculation process. Choose an Azure OpenAI connection and a deployment with either GPT-3.5, GPT-4, or the Davinci model for our calculations.

![image](https://github.com/user-attachments/assets/9b76e5dd-669a-49fe-b1ac-5b594942771b)

AI Quality (NLP) metrics are mathematically based measurements that assess your application's performance. They often require ground truth data for calculation. ROUGE is a family of metrics. You can select the ROUGE type to calculate the scores. Various types of ROUGE metrics offer ways to evaluate the quality of text generation. ROUGE-N measures the overlap of n-grams between the candidate and reference texts.

![image](https://github.com/user-attachments/assets/00532abc-2487-495e-9bf3-0d6b78bbd016)

For risk and safety metrics, you don't need to provide a connection and deployment. The Azure AI Foundry portal safety evaluations back-end service provisions a GPT-4 model that can generate content risk severity scores and reasoning to enable you to evaluate your application for content harms.

You can set the threshold to calculate the defect rate for the content harm metrics (self-harm-related content, hateful and unfair content, violent content, sexual content). The defect rate is calculated by taking a percentage of instances with severity levels (Very low, Low, Medium, High) above a threshold. By default, we set the threshold as "Medium".

For protected material and indirect attack, the defect rate is calculated by taking a percentage of instances where the output is 'true' (Defect Rate = (#trues / #instances) × 100).

![image](https://github.com/user-attachments/assets/a7ff921b-73c9-4d73-bac9-5a9763d2161b)

Data mapping for evaluation: You must specify which data columns in your dataset correspond with inputs needed in the evaluation. Different evaluation metrics demand distinct types of data inputs for accurate calculations.

![image](https://github.com/user-attachments/assets/e60cc2b3-34e9-40c7-be25-70ff55b88022)

### Review and finish

After completing all the necessary configurations, you can review and proceed to select 'Submit' to submit the evaluation run.

![image](https://github.com/user-attachments/assets/b0061612-bbb4-4a38-b213-ca8df3fa9aef)

### Model and prompt evaluation
To create a new evaluation for your selected model deployment and defined prompt, use the simplified model evaluation panel. This streamlined interface allows you to configure and initiate evaluations within a single, consolidated panel.

### Basic information
To start, you can set up the name for your evaluation run. Then select the model deployment you want to evaluate. We support both Azure OpenAI models and other open models compatible with Model-as-a-Service (MaaS), such as Meta Llama and Phi-3 family models. Optionally, you can adjust the model parameters like max response, temperature, and top P based on your need.

In the System message text box, provide the prompt for your scenario. For more information on how to craft your prompt, see the prompt catalog. You can choose to add example to show the chat what responses you want. It will try to mimic any responses you add here to make sure they match the rules you laid out in the system message.

![image](https://github.com/user-attachments/assets/41266abc-5c04-48c2-9546-dc7cfea23f7c)

### Configure test data
After configuring the model and prompt, set up the test dataset that will be used for evaluation. This dataset will be sent to the model to generate responses for assessment. You have three options for configuring your test data:
- Generate sample data
- Use existing dataset
- Add your dataset
If you don't have a dataset readily available and would like to run an evaluation with a small sample, you can select the option to use a GPT model to generate sample questions based on your chosen topic. The topic helps tailor the generated content to your area of interest. The queries and responses will be generated in real time, and you have the option to regenerate them as needed.

![image](https://github.com/user-attachments/assets/92aefa40-78f4-4d0e-9469-49111734b4f6)

### Data mapping
If you choose to use an existing dataset or upload a new dataset, you'll need to map your dataset’s columns to the required fields for evaluation. During evaluation, the model’s response will be assessed against key inputs such as:
- Query: required for all metrics
- Context: optional
- Ground Truth: optional, required for AI quality (NLP) metrics
These mappings ensure accurate alignment between your data and the evaluation criteria.

### Choose evaluation metrics
The last step is to select what you’d like to evaluate. Instead of selecting individual metrics and having to familiarize yourself with all the options available, we simplify the process by allowing you to select metric categories that best meet your needs. When you choose a category, all relevant metrics within that category will be calculated based on the data columns you provided in the previous step. Once you select the metric categories, you can select “Create” to submit the evaluation run and go to the evaluation page to see the results.

We support three categories:
- AI quality (AI assisted): You need to provide an Azure OpenAI model deployment as the judge to calculate the AI assisted metrics.
- AI quality (NLP)
- Safety

  | **AI Quality (AI assisted)**    | **AI quality (NLP)** | **Safety**                  |
|------------------------------|----------------------|-----------------------------|
| Groundedness (require context) | F1 score             | Self-harm-related content   |
| Relevance (require context)  | ROUGE score          | Hateful and unfair content  |
| Coherence                    | BLEU score           | Violent content             |
| Fluency                      | GLEU score           | Sexual content              |
|                              | METEOR score         | Protected material          |
|                              |                      | Indirect attack             |


### Create an evaluation with custom evaluation flow
You can develop your own evaluation methods:
From the flow page: From the collapsible left menu, select Prompt flow > Evaluate > Custom evaluation.

![image](https://github.com/user-attachments/assets/7b8e70c1-8140-44a2-a5a1-8f5d94ab1da5)

### View and manage the evaluators in the evaluator library

The evaluator library is a centralized place that allows you to see the details and status of your evaluators. You can view and manage Microsoft curated evaluators.

The evaluator library also enables version management. You can compare different versions of your work, restore previous versions if needed, and collaborate with others more easily.

To use the evaluator library in Azure AI Foundry portal, go to your project's Evaluation page and select the Evaluator library tab.

![image](https://github.com/user-attachments/assets/c256f407-397b-4d37-9fc4-43e4302afd8b)

You can select the evaluator name to see more details. You can see the name, description, and parameters, and check any files associated with the evaluator. Here are some examples of Microsoft curated evaluators:
- For performance and quality evaluators curated by Microsoft, you can view the annotation prompt on the details page. You can adapt these prompts to your own use case by changing the parameters or criteria according to your data and objectives Azure AI Evaluation SDK. For example, you can select Groundedness-Evaluator and check the Prompty file showing how we calculate the metric.
- For risk and safety evaluators curated by Microsoft, you can see the definition of the metrics. For example, you can select the Self-Harm-Related-Content-Evaluator and learn what it means and how Microsoft determines the various severity levels for this safety metric.

# How to view evaluation results in Azure AI Foundry portal

The Azure AI Foundry portal evaluation page is a versatile hub that not only allows you to visualize and assess your results but also serves as a control center for optimizing, troubleshooting, and selecting the ideal AI model for your deployment needs. It's a one-stop solution for data-driven decision-making and performance enhancement in your Azure AI Foundry projects. You can seamlessly access and interpret the results from various sources, including your flow, the playground quick test session, evaluation submission UI, and SDK. This flexibility ensures that you can interact with your results in a way that best suits your workflow and preferences.

Once you've visualized your evaluation results, you can dive into a thorough examination. This includes the ability to not only view individual results but also to compare these results across multiple evaluation runs. By doing so, you can identify trends, patterns, and discrepancies, gaining invaluable insights into the performance of your AI system under various conditions.

In this article you learn to:
- View the evaluation result and metrics.
- Compare the evaluation results.
- Understand the built-in evaluation metrics.
- Improve the performance.
- View the evaluation results and metrics.

## Find your evaluation results

Upon submitting your evaluation, you can locate the submitted evaluation run within the run list by navigating to the Evaluation page.

You can monitor and manage your evaluation runs within the run list. With the flexibility to modify the columns using the column editor and implement filters, you can customize and create your own version of the run list. Additionally, you can swiftly review the aggregated evaluation metrics across the runs, enabling you to perform quick comparisons.

![image](https://github.com/user-attachments/assets/181585ae-32e1-41ed-a4f1-1ed819bec48b)

For a deeper understanding of how the evaluation metrics are derived, you can access a comprehensive explanation by selecting the 'Learn more about metrics' option. This detailed resource provides valuable insights into the calculation and interpretation of the metrics used in the evaluation process.

![image](https://github.com/user-attachments/assets/ae9c2794-1729-4d87-9242-77637e8a030b)

You can choose a specific run, which will take you to the run detail page. Here, you can access comprehensive information, including evaluation details such as test dataset, task type, prompt, temperature, and more. Furthermore, you can view the metrics associated with each data sample. The metrics scores charts provide a visual representation of how scores are distributed for each metric throughout your dataset.

## Metric dashboard charts
We break down the aggregate views with different types of your metrics by AI Quality (AI assisted), Risk and safety, AI Quality (NLP), and Custom when applicable. You can view the distribution of scores across the evaluated dataset and see aggregate scores for each metric.
- For AI Quality (AI assisted), we aggregate by calculating an average across all the scores for each metric. If you calculate Groundedness Pro, the output is binary and so the aggregated score is passing rate, which is calculated by (#trues / #instances) × 100.
![image](https://github.com/user-attachments/assets/fd313f74-45ee-4452-9b6f-ffcb713f7a5c)
- For risk and safety metrics, we aggregate by calculating a defect rate for each metric.
  - For content harm metrics, the defect rate is defined as the percentage of instances in your test dataset that surpass a threshold on the severity scale over the whole dataset size. By default, the threshold is “Medium”.
  - For protected material and indirect attack, the defect rate is calculated as the percentage of instances where the output is 'true' (Defect Rate = (#trues / #instances) × 100).
![image](https://github.com/user-attachments/assets/7fbdb2c5-8fb1-4c43-8ce3-c9e2f73a984e)
- For AI Quality (NLP) metrics, we show histogram of the metric distribution between 0 and 1. We aggregate by calculating an average across all the scores for each metric.
![image](https://github.com/user-attachments/assets/0787f181-3fd3-4c3f-b41a-0183d65432a8)
- For custom metrics, you can select Add custom chart, to create a custom chart with your chosen metrics or to view a metric against selected input parameters.
![image](https://github.com/user-attachments/assets/fa37ce91-8336-4d7e-844d-a58b77346bcb)

You can also customize existing charts for built-in metrics by changing the chart type.
![image](https://github.com/user-attachments/assets/6f32f1da-c069-4e95-acc6-362844ba7bcf)

## Detailed metrics result table
Within the metrics detail table, you can conduct a comprehensive examination of each individual data sample. Here, you can scrutinize the generated output and its corresponding evaluation metric score. This level of detail enables you to make data-driven decisions and take specific actions to improve your model's performance.

Some potential action items based on the evaluation metrics could include:
- Pattern Recognition: By filtering for numerical values and metrics, you can drill down to samples with lower scores. Investigate these samples to identify recurring patterns or issues in your model's responses. For instance, you might notice that low scores often occur when the model generates content on a certain topic.
- Model Refinement: Use the insights from lower-scoring samples to improve the system prompt instruction or fine-tune your model. If you observe consistent issues with, for example, coherence or relevance, you can also adjust the model's training data or parameters accordingly.
- Column Customization: The column editor empowers you to create a customized view of the table, focusing on the metrics and data that are most relevant to your evaluation goals. This can streamline your analysis and help you spot trends more effectively.
- Keyword Search: The search box allows you to look for specific words or phrases in the generated output. This can be useful for pinpointing issues or patterns related to particular topics or keywords and addressing them specifically.

The metrics detail table offers a wealth of data that can guide your model improvement efforts, from recognizing patterns to customizing your view for efficient analysis and refining your model based on identified issues.

Here are some examples of the metrics results for the question answering scenario:
![image](https://github.com/user-attachments/assets/3e328f43-1c41-43ab-8d32-a8a30574235e)

And here are some examples of the metrics results for the conversation scenario:
![image](https://github.com/user-attachments/assets/539ca4ea-8985-43d2-b357-742fa79fc9bc)

For multi-turn conversation scenario, you can select “View evaluation results per turn” to check the evaluation metrics for each turn in a conversation.
![image](https://github.com/user-attachments/assets/ce3efa54-89d9-484f-83d9-8c5c04b73e59)
![image](https://github.com/user-attachments/assets/171a7fd5-7737-45fc-a934-dffba95650ee)

For a safety evaluation in a multi-modal scenario (text + images), you can review the images from both the input and output in the detailed metrics result table to better understand the evaluation result. Since multi-modal evaluation is currently supported only for conversation scenarios, you can select "View evaluation results per turn" to examine the input and output for each turn.
![image](https://github.com/user-attachments/assets/8fec099c-d80e-4f78-aff9-e6619822e650)
![image](https://github.com/user-attachments/assets/d51dcac2-e5eb-4e92-811d-c9784d065563)

Select the image to expand and view it. By default, all images are blurred to protect you from potentially harmful content. To view the image clearly, turn on the "Check Blur Image" toggle.
![image](https://github.com/user-attachments/assets/cbdbef0e-3f16-4955-ae0a-6c074954c3d4)

For risk and safety metrics, the evaluation provides a severity score and reasoning for each score. Here are some examples of risk and safety metrics results for the question answering scenario:
![image](https://github.com/user-attachments/assets/f52a22b7-249b-4b8e-bfec-378badf84c90)

Evaluation results might have different meanings for different audiences. For example, safety evaluations might generate a label for “Low” severity of violent content that may not align to a human reviewer’s definition of how severe that specific violent content might be. We provide a human feedback column with thumbs up and thumbs down when reviewing your evaluation results to surface which instances were approved or flagged as incorrect by a human reviewer.
![image](https://github.com/user-attachments/assets/745f91e9-edaa-42c4-984d-3ee781d138db)

When understanding each content risk metric, you can easily view each metric definition and severity scale by selecting on the metric name above the chart to see a detailed explanation in a pop-up.
![image](https://github.com/user-attachments/assets/fe9ccec7-2824-44a6-9327-b94cc2be9036)

If there's something wrong with the run, you can also debug your evaluation run with the logs.

Here are some examples of the logs that you can use to debug your evaluation run:
![image](https://github.com/user-attachments/assets/50fb3ac0-9b91-4b24-a712-4370b84107fe)

If you're evaluating a prompt flow, you can select the View in flow button to navigate to the evaluated flow page to make update to your flow. For example, adding additional meta prompt instruction, or change some parameters and re-evaluate.

## Manage and share view with view options
On the Evaluation Details page, you can customize your view by adding custom charts or editing columns. Once customized, you have the option to save the view and/or share it with others using the view options. This enables you to review evaluation results in a format tailored to your preferences and facilitates collaboration with colleagues.
![image](https://github.com/user-attachments/assets/e98ce928-c8ec-4a71-b3b4-0b544226e86d)

## Compare the evaluation results
To facilitate a comprehensive comparison between two or more runs, you have the option to select the desired runs and initiate the process by selecting either the Compare button or, for a general detailed dashboard view, the Switch to dashboard view button. This feature empowers you to analyze and contrast the performance and outcomes of multiple runs, allowing for more informed decision-making and targeted improvements.
![image](https://github.com/user-attachments/assets/2bcc7440-9bb7-4efe-9d33-82f1cbcbca7c)

In the dashboard view, you have access to two valuable components: the metric distribution comparison chart and the comparison table. These tools enable you to perform a side-by-side analysis of the selected evaluation runs, allowing you to compare various aspects of each data sample with ease and precision.
![image](https://github.com/user-attachments/assets/99f4712c-fc41-489b-b7ee-936df4f0f23f)

Within the comparison table, you have the capability to establish a baseline for your comparison by hovering over the specific run you wish to use as the reference point and set as baseline. Moreover, by activating the 'Show delta' toggle, you can readily visualize the differences between the baseline run and the other runs for numerical values. Additionally, with the 'Show only difference' toggle enabled, the table displays only the rows that differ among the selected runs, aiding in the identification of distinct variations.

Using these comparison features, you can make an informed decision to select the best version:
- Baseline Comparison: By setting a baseline run, you can identify a reference point against which to compare the other runs. This allows you to see how each run deviates from your chosen standard.
- Numerical Value Assessment: Enabling the 'Show delta' option helps you understand the extent of the differences between the baseline and other runs. This is useful for evaluating how various runs perform in terms of specific evaluation metrics.
- Difference Isolation: The 'Show only difference' feature streamlines your analysis by highlighting only the areas where there are discrepancies between runs. This can be instrumental in pinpointing where improvements or adjustments are needed.

By using these comparison tools effectively, you can identify which version of your model or system performs the best in relation to your defined criteria and metrics, ultimately assisting you in selecting the most optimal option for your application.
![image](https://github.com/user-attachments/assets/36731ba8-ab5b-4725-b75e-92ecae524357)

## Measuring jailbreak vulnerability
Evaluating jailbreak is a comparative measurement, not an AI-assisted metric. Run evaluations on two different, red-teamed datasets: a baseline adversarial test dataset versus the same adversarial test dataset with jailbreak injections in the first turn. You can use the adversarial data simulator to generate the dataset with or without jailbreak injections.

To understand if your application is vulnerable to jailbreak, you can specify which is the baseline and then turn on “Jailbreak defect rates” toggle in the comparison table. Jailbreak defect rate is defined as the percentage of instances in your test dataset where a jailbreak injection generated a higher severity score for any content risk metric with respect to a baseline over the whole dataset size. You can select multiple evaluations in your compare dashboard to view the difference in defect rates.
![image](https://github.com/user-attachments/assets/d1376b3d-5425-4edf-9504-cedd393f5025)

## Understand the built-in evaluation metrics
Understanding the built-in metrics is vital for assessing the performance and effectiveness of your AI application. By gaining insights into these key measurement tools, you're better equipped to interpret the results, make informed decisions, and fine-tune your application to achieve optimal outcomes. To learn more about the significance of each metric, how it's being calculated, its role in evaluating different aspects of your model, and how to interpret the results to make data-driven improvements, refer to Evaluation and Monitoring Metrics.

# How to run evaluations online with the Azure AI Foundry SDK

In this article, you learn how to run evaluations online in a continuous manner with the Azure AI Foundry SDK. Evaluations in pre-production environments is essential for ensuring that your application is safe, performant, and produces high-quality results. However, evaluation doesn't stop after your application is deployed. In production, various things can change, such as the types of queries users are sending to your application, which can influence your application's performance. To maintain a high degree of observability into your production generative AI application, it's important to trace and continuously evaluate your application's data. By doing so, you can maintain confidence in your application's safety, quality, and performance.

How online evaluation works
In this section, you'll learn how online evaluation works, how it integrates with Azure Monitor Application Insights, and how you can use it to run continuous evaluations over trace data from your generative AI applications.

After your application is instrumented to send trace data to Application Insights, set up an online evaluation schedule to continuously evaluate this data. Online evaluation is a service that uses Azure AI compute to continuously run a configurable set of evaluators. After you have set up an online evaluation schedule with the Azure AI Foundry SDK, it runs on a configurable schedule. Each time the scheduled job runs, it performs the following steps:
1. Query application trace data from the connected Application Insights resource using provided Kusto (KQL) query.
2. Run each evaluator over the trace data and calculate each metric (for example, groundedness: 3).
3. Write evaluation scores back to each trace using standardized semantic conventions.

For example, let’s say you have a deployed chat application that receives many customer questions on a daily basis. You want to continuously evaluate the quality of the responses from your application. You set up an online evaluation schedule with a daily recurrence. You configure the evaluators: Groundedness, Coherence, and Fluency. Every day, the service computes the evaluation scores for these metrics and writes the data back to Application Insights for each trace that was collected during the recurrence time window (in this example, the past 24 hours). Then, the data can be queried from each trace and made accessible in Azure AI Foundry and Azure Monitor Application Insights.

The evaluation results written back to each trace within Application Insights follow the following conventions. A unique span is added to each trace for each evaluation metric:

| **Property**                 | **Application Insights Table** | **Fields for a given operation_ID** | **Example value**                |
|------------------------------|--------------------------------|-------------------------------------|----------------------------------|
| Evaluation metric            | traces, AppTraces              | customDimensions["event.name"]      | gen_ai.evaluation.relevance      |
| Evaluation metric score      | traces, AppTraces              | customDimensions["gen_ai.evaluation.score"] | 3                          |
| Evaluation metric comment (if applicable) | traces, AppTraces              | message                             | {"comment": "I like the response"} |

Now that you understand how online evaluation works and how it connects to Azure Monitor Application Insights, the next step is to set up the service.

## Set up online evaluation
In this section, you'll learn how to configure an online evaluation schedule to continuously monitor your deployed generative AI application. Azure AI Foundry SDK offers such capabilities via. A Python API and supports all of the features available in local evaluations. Use the following steps to submit your online evaluation schedule on your data using built-in or custom evaluators.

## Prerequisites
Complete the following prerequisite steps to set up your environment and authentication to the necessary resources:
- An Azure Subscription.
- A Resource Group in an Evaluation-supported region.
- A new User-assigned Managed Identity in the same resource group and region. Make a note of the clientId; you'll need it later.
- An Azure AI Hub in the same resource group and region.
- An Azure AI project in this hub, see Create a project in Azure AI Foundry portal.
- An Azure Monitor Application Insights resource.
- Navigate to the hub page in Azure portal and add Application Insights resource, see Update Azure Application Insights and Azure Container Registry.
- Azure OpenAI Deployment with GPT model supporting chat completion, for example gpt-4.
- Connection String for Azure AI project to easily create AIProjectClient object. You can get the Project connection string under Project details from the project's Overview page.
- Navigate to your Application Insights resource in the Azure portal and use the Access control (IAM) tab to add the Log Analytics Contributor role to the User-assigned Managed Identity you created previously.
- Attach the User-assigned Managed Identity to your project.
- Navigate to your Azure AI Services in the Azure portal and use the Access control (IAM) tab to add the Cognitive Services OpenAI Contributor role to the User-assigned Managed - Identity you created previously.













