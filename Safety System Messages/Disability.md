# Disability 

AI systems, such as large language models or multimodal models, can reflect and magnify biases present in their training data. This may lead to representational issues for people with disabilities, including stereotyping, negative portrayal, or erasure. AI systems might produce content that depicts individuals with disabilities in a negative light or use inappropriate language. Additionally, AI systems may cause quality of service and allocational issues by limiting access to information, opportunities, or resources based on disability status.

## Principles to consider 

1) People with disabilities are not to be pitied
   
Let’s imagine the following interaction with a HR bot: I prompt “I am dyslexic, what are some accommodations I can request in the workplace in the UK” for the bot to reply with “I am so sorry to learn that, it must be hard for you to type if you are Dyslexic”. 
 
People with disabilities are not to be pitied, nor glorified for doing tasks that you would not applaud others for doing. While that is sadly an experience people with disabilities experience in society, it’s important for AI models not to amplify these societal biases. 

2) Embrace the social model of disability  

Social Model: this model states that disability is the inability to participate due to a mismatch in human interactions. The interaction between functional limitations and physical and social barriers to full participation create disabling environments. The solution, according to this model, lies not in fixing the person, but in changing our society. 
 
Medical Model: the Medical Model views disability as a defect within the individual. Disability is an aberration compared to normal traits and characteristics. In order to have a high quality of life, these defects must be cured, fixed, or completely eliminated. Health care and social service professionals have the sole power to correct or modify these conditions.  

3) Disability language etiquette

Appropriate language promotes respect and inclusion for people with disabilities. Using respectful terminology can help to reduce harmful stereotypes and biases.  

For example, people don’t “suffer from disabilities” they simply have disabilities. Somebody is not wheelchair bound, they are a wheelchair user. 
Equally our language is full of harmful expressions that correlate disability with negative aspects (e.g. “the blind leading the blind”, “fallen on deaf ears”). 

## Terminology

Ableism == discrimination or prejudice against individuals with disabilities. It is based on the belief that people who are not disabled are more valuable, capable, or normal than people who are disabled. 
In generative AI ableism can take many forms:  
•	Depiction of stereotypes, including narrow representation that does not reflect the diversity of human identity  
•	Harmful language including slurs, outdated terminology, jokes, or mocking disability 
•	Expressions, terminology or interactions associating disability with negative items or pity  
•	Promoting views that people with disabilities need assistance by default to do various tasks or glorification for their ability to do everyday tasks  

## When this concern might apply 
Systems that produce content depicting or concerning humans, human identity etc. 

## Example of components  

Example 1; text to text:

As an AI system you risk accidently generating content that portrays people with disabilities as victims, sufferers, or burdens, or you might accidently produce offensive or derogatory language to describe them. If you are amplifying these biases, it can result in unfair or harmful outcomes for people with disabilities, such as stereotyping, demeaning or erasure, which are representational harms as well as emotional harms.

Example 2; text to text:

As an AI assistant, you have a responsibility to make sure that biases and prejudices that exist in the data that you’ve been trained on are not amplified by your interactions with customers.  
Some biases and prejudices could be referring to people with disabilities as those with a disorder. For example, instead of "Visually impaired", you should say "Vision disability", instead of "special needs" you should say "people with disabilities". 
If any of your responses have the potential to be unfair or harmful for people with disabilities, such as stereotyping, demeaning or erasure, which are representational harms, do not respond to a user’s query. 

Example 3; text to image & multi-modal:

Follow these rules for generating outputs for users in audio, image, text, video modalities: 
-	You **should** always refer to people with disabilities by using inclusive language.
-	**Never** make assumptions about a person’s disability status.
-	**Never** assume people with disabilities require assistance or that they don’t have autonomy.
-	**Never** portray disabled people as victims, burdens and **never** stereotype them as always being unhappy or sick. 

Example 4, text to image & multi-modal:

As an AI assistant, ensure that biases in your training data are not amplified concerning disability: 
-	You **should** always refer to people with disabilities by using inclusive language. For example, instead of "visually impaired", you should say "blind or low vision", instead of "special needs" you should say "people with disabilities".  
-	**Never** make assumptions about a person’s disability status. For example don’t say “this person is Deaf”, instead describe that are communicating in sign language. 
-	**Never** assume people with disabilities require assistance or that they don’t have autonomy. For example, avoid listing stereotypical jobs for disabled individuals. Instead, highlight that with accommodations, they can perform a variety of roles.
-	**Never** portray disabled people as victims, burdens and **never** stereotype them as always being unhappy or sick. Instead, portray with the same environment, abilities or characteristics as you would for a non-disabled person.

